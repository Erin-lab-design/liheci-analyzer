#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
generate_liheci_split_xfst_v3.py

Automatically generates liheci_split.xfst from liheci_lexicon.csv

Compile with:
    hfst-xfst -F liheci_split.xfst

Generates:
    liheci_split.generator.hfst   (lemma-tag → sentence)
    liheci_split.analyser.hfst    (sentence → lemma-tag)

Output tag examples:
    散心+Lemma+Verb-Object+WHOLE
    散心+Lemma+Verb-Object+SPLIT
    散心+Lemma+Verb-Object+SPLIT+REDUP
"""

import csv
from pathlib import Path

INPUT_CSV = "data/liheci_lexicon.csv"
OUTPUT_XFST = "scripts/hfst_files/liheci_split.xfst"


def map_type_tag(type_str: str) -> str:
    """
    Map Type from CSV to HFST type tag (no spaces).
    Preserve original semantics: Verb-Object, Pseudo V-O, Modifier-Head, Simplex Word
    """
    t = (type_str or "").strip()
    if not t:
        return "UnknownType"
    # Remove spaces, keep dashes
    return t.replace(" ", "")


def has_redup(row: dict) -> bool:
    """
    Check if this lemma supports reduplication form (REDUP)
    Check RedupPattern column first, fallback to Notes column
    """
    # Use RedupPattern column first
    redup_pattern = (row.get("RedupPattern") or "").strip()
    if redup_pattern and "AAB" in redup_pattern:
        return True
    
    # Fallback to Notes column (for backward compatibility)
    notes = (row.get("Notes") or "").strip()
    if notes and "AAB" in notes:
        return True
    
    return False


def chars_with_space(s: str) -> str:
    """Split string into individual characters separated by spaces for xfst regex."""
    s = (s or "").strip()
    if not s:
        return ""
    return " ".join(list(s))


def main():
    input_path = Path(INPUT_CSV)
    if not input_path.exists():
        raise SystemExit(f"ERROR: Cannot find {INPUT_CSV}, please check the filename and path.")

    rows = []
    with input_path.open(encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            lemma = (row.get("Lemma") or "").strip()
            head = (row.get("A") or "").strip()
            tail = (row.get("B") or "").strip()
            # Skip incomplete rows (e.g., a single 'c' line)
            if not lemma or not head or not tail:
                continue
            rows.append(row)

    lines = []

    # =======================
    # 1) Common definitions
    # =======================
    lines.append("! Auto-generated by generate_liheci_split_xfst_v3.py")
    lines.append("! Sentence-level Liheci WHOLE / SPLIT / REDUP recognizer")
    lines.append("")

    lines.append("! 1) Symbol sets")
    lines.append(
        'define Punct [， | 。 | 、 | ！ | ？ | ： | ； | "，" | "。" | "," | "." | "!" | "?"];'
    )
    lines.append("define AnyChar [?];")
    lines.append("define LegalIns [AnyChar - Punct];")
    # X in A_XAB pattern, currently supports "一" and "了"
    lines.append("define RedupMid [一 | 了];")
    lines.append("")

    all_transducer_names = []

    # =======================
    # 2) Generate pattern + transducer for each lemma
    # =======================
    for idx, row in enumerate(rows, start=1):
        lemma = (row.get("Lemma") or "").strip()
        head = (row.get("A") or "").strip()
        tail = (row.get("B") or "").strip()
        type_str = (row.get("Type") or "").strip()

        type_tag = map_type_tag(type_str)

        head_chars = chars_with_space(head)
        tail_chars = chars_with_space(tail)
        if not head_chars or not tail_chars:
            continue

        base = f"L{idx:03d}"  # L001, L002, ...

        # Upper-side labels
        # Only generate WHOLE and SPLIT forms
        # REDUP forms will be handled in a separate pipeline stage
        whole_upper = f"{lemma}+Lemma+{type_tag}+WHOLE"
        split_upper = f"{lemma}+Lemma+{type_tag}+SPLIT"

        whole_pat_name = f"{base}WholePat"
        split_pat_name = f"{base}SplitPat"
        whole_tr_name = f"{base}Whole"
        split_tr_name = f"{base}Split"

        lines.append(f"! === {lemma} (Type={type_str}) ===")

        # --- WHOLE pattern: H T with NO intervening characters (head immediately followed by tail) ---
        # This is mutually exclusive with SPLIT by definition
        lines.append(f"define {whole_pat_name} ?* {head_chars} {tail_chars} ?* ;")
        
        # --- SPLIT pattern: H LegalIns+ T (at least one non-punctuation insertion between H and T) ---
        # Mutually exclusive with WHOLE (requires at least one character between H and T)
        lines.append(
            f"define {split_pat_name} ?* {head_chars} LegalIns LegalIns* {tail_chars} ?* ;"
        )

        # Bind transducers (no subtraction needed - patterns are naturally mutually exclusive)
        lines.append(f'define {whole_tr_name} "{whole_upper}" : {whole_pat_name} ;')
        lines.append(f'define {split_tr_name} "{split_upper}" : {split_pat_name} ;')

        transducers_for_this = [whole_tr_name, split_tr_name]

        lines.append("")

        all_transducer_names.extend(transducers_for_this)

    # =======================
    # 3) Union + save
    # =======================
    lines.append("! Combine all lemma transducers")
    if all_transducer_names:
        union_expr = " | ".join(all_transducer_names)
        lines.append("regex " + union_expr + " ;")
    else:
        lines.append("regex [] ; ! No lemmas, empty language")

    lines.append("")
    lines.append("! Stack top is now generator:")
    lines.append("! Upper: <Lemma>+Lemma+TypeTag+FORM(WHOLE|SPLIT)")
    lines.append("! Lower: sentences matching corresponding pattern")
    lines.append("save stack liheci_split.generator.hfst")
    lines.append("")

    lines.append("invert net")
    lines.append("save stack liheci_split.analyser.hfst")
    lines.append("")

    lines.append("quit")
    lines.append("")

    Path(OUTPUT_XFST).write_text("\n".join(lines), encoding="utf-8")
    print(f"[OK] Generated {OUTPUT_XFST}, total {len(rows)} lemmas")


if __name__ == "__main__":
    main()
