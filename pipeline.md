# Liheci HFST Analysis Pipeline

## Overview

This pipeline uses a two-stage HFST approach to recognize and validate Chinese separable verbs (离合词) in natural text.

## Pipeline Architecture

```
Input: Test Sentences (data/test_sentences.txt)
    ↓
┌─────────────────────────────────────────────────────────────┐
│ Stage 1: WHOLE/SPLIT Recognition                            │
│ Script: 03.liheci_run_hfst.py                               │
│ FST: liheci_split.analyser.hfst (131 lemmas, 663 states)   │
├─────────────────────────────────────────────────────────────┤
│ Generated by: 01.generate_liheci_split_xfst.py             │
│                                                              │
│ Recognizes:                                                  │
│   ├─ WHOLE: contiguous form (睡觉)                          │
│   └─ SPLIT: inserted form (睡了一觉, 睡一个好觉)            │
│                                                              │
│ Pattern: ?* A [Insertion] B ?*                              │
│   where [Insertion] can be:                                 │
│     - Aspect markers: 了, 过, 着                            │
│     - Quantifiers: 一, 两, 一个, 三次, etc.                 │
│     - Modifiers: 个, 好, 完, etc.                           │
│                                                              │
│ Output: outputs/liheci_hfst_outputs.tsv (328 rows)         │
│         outputs/liheci_hfst_run.log                         │
│ TSV Columns: sent_id, gold_stem, gold_label, error_type,   │
│              sentence, lemma, type_tag, shape, hfst_analysis│
└─────────────────────────────────────────────────────────────┘
    ↓
┌─────────────────────────────────────────────────────────────┐
│ Stage 2: REDUP Validation                                   │
│ Script: 04.liheci_validate_redup_hfst.py                    │
│ FST: liheci_redup.analyser.hfst (55 AAB lemmas, 225 states)│
├─────────────────────────────────────────────────────────────┤
│ Generated by: 02.generate_liheci_redup_xfst.py             │
│                                                              │
│ Process:                                                     │
│ 1. Read Stage 1 output (325 rows)                          │
│ 2. Deduplicate by (sent_id, lemma, shape) → 313 rows       │
│ 3. Identify WHOLE+SPLIT cooccurrence → 124 potential REDUP │
│ 4. Run sentences through REDUP FST                         │
│ 5. Validate AAB patterns:                                   │
│      ├─ AAB: 散散步                                         │
│      ├─ A一AB: 散一散步                                     │
│      └─ A了AB: 散了散步                                     │
│ 6. Filter out invalid reduplications                        │
│                                                              │
│ Results:                                                     │
│   ├─ Valid REDUP: 18 lemmas                                │
│   ├─ Invalid REDUP: 90 rows filtered                       │
│   └─ Non-REDUP: 190 rows preserved                         │
│                                                              │
│ Output: outputs/liheci_hfst_outputs.tsv (208 rows)         │
│         outputs/liheci_redup_validation.log                 │
└─────────────────────────────────────────────────────────────┘
    ↓
Current Results: 206 valid liheci recognitions
    ├─ 188 non-reduplication cases (WHOLE or SPLIT)
    └─ 18 valid reduplication cases (REDUP)
    ↓
┌─────────────────────────────────────────────────────────────┐
│ Stage 3: Insertion Component Analysis with HFST            │
│ Scripts: 05.generate_insertion_context_xfst.py             │
│          06.stage3_insertion_analysis.py                   │
│ FST: liheci_insertion_annotator.hfst (43 states, 123 arcs) │
├─────────────────────────────────────────────────────────────┤
│ Architecture:                                                │
│   XFST annotates entire sentence → Python extracts tags    │
│                                                              │
│ XFST Replace Rules (Character-level Annotation):           │
│   Input: 昨天晚上我睡<HEAD>了一个好<TAIL>觉。              │
│   Output: 昨天晚上我:PRO+睡<HEAD>了:ASPECT+一:NUM+个:CLF+  │
│           好:MOD+<TAIL>觉。                                 │
│                                                              │
│ Component Tags (8 categories):                              │
│   ├─ ASPECT: 了, 过, 着 (aspect markers)                  │
│   ├─ NUM: 一, 二, 两, 三...半 (numerals, 16 items)        │
│   ├─ CLF: 个, 次, 天, 把...回 (classifiers, 18 items)     │
│   ├─ PRO: 我, 你, 他, 她, 它 (pronouns)                   │
│   ├─ DE: 的 (structural particle)                          │
│   ├─ MOD: 好, 大, 小, 很...整 (modifiers, 11 items)       │
│   ├─ RES: 完, 到 (resultatives)                           │
│   └─ PREP: 跟, 和, 与, 向...把 (prepositions, 9 items)    │
│                                                              │
│ Python Extraction Logic:                                    │
│   ├─ SPLIT: Extract tags between <HEAD> and <TAIL>         │
│   │    Example: 了:ASPECT+一:NUM+个:CLF+好:MOD+            │
│   └─ WHOLE: Extract tags before <HEAD> (last 50 chars)     │
│        Check for PREP+PRO pattern (external PP)            │
│                                                              │
│ Insertion Type Classification:                              │
│   ├─ ASPECT_QUANT: ASPECT + NUM + CLF                     │
│   ├─ ASPECT: ASPECT only                                   │
│   ├─ QUANTIFIER: NUM + CLF or CLF only                    │
│   ├─ PRONOUN_DE: PRO + DE                                 │
│   ├─ PRONOUN: PRO only                                     │
│   ├─ MODIFIER_DE: MOD + DE                                │
│   ├─ MODIFIER: MOD only                                    │
│   ├─ RESULTATIVE: RES                                      │
│   ├─ EXT_PP: WHOLE with external PREP+PRO                 │
│   ├─ EMPTY: No insertion                                   │
│   └─ UNKNOWN: Unrecognized pattern                        │
│                                                              │
│ Confidence Scoring (Coverage-based):                        │
│   confidence = tagged_chars / total_chars                   │
│   Example: "了一个好" → 4/4 = 1.00 (100% coverage)        │
│            "个热水" → 1/3 = 0.33 (33% coverage)           │
│                                                              │
│ Error Detection:                                             │
│   ├─ MISSING_DE: Lemmas requiring "的" without it         │
│   │    (捣乱, 丢脸, 造反, 革命) → confidence *= 0.3      │
│   └─ PP_POS: Lemmas with misplaced prepositional phrase   │
│        (道歉, 道谢, 拜年, 见面...) → confidence *= 0.2    │
│                                                              │
│ Output Columns:                                              │
│   [existing columns] + insertion + insertion_tagged +       │
│   insertion_type + has_de + error_type + confidence_score   │
│                                                              │
│ Output: outputs/liheci_insertion_analysis.tsv (206 rows)   │
└─────────────────────────────────────────────────────────────┘
    ↓
Results: 206 rows with insertion analysis
    ├─ 181 classified insertions (88% coverage)
    ├─ 12 UNKNOWN insertions (6%)
    ├─ 18 REDUP_SKIP (9%)
    ├─ 10 PP_POS errors detected
    ├─ 5 MISSING_DE errors detected
    └─ Average confidence: 0.62
    ↓
[Stage 4] Threshold-based Filtering (Optional)
    ├─ Filter candidates with confidence < 0.40
    ├─ Manual review for 0.40-0.60 range
    └─ Auto-accept confidence ≥ 0.60
    ↓
High-confidence candidates (~150-180 rows)
    ↓
[Stage 5] Python Semantic Validation (Future)
    ├─ HanLP POS tagging verification
    ├─ Transitivity and PP requirement checking
    ├─ Semantic role analysis for pronoun insertion
    └─ Cross-sentence context analysis
    ↓
Final validated results

## File Descriptions

### Data Files
- `data/liheci_lexicon.csv` - Main lexicon (131 lemmas)
  - Contains: Lemma, A, B, Type, Pinyin, Definition, RedupPattern
  - 55 lemmas marked as RedupPattern='AAB'
- `data/test_sentences.txt` - Test sentences with gold annotations

### Stage 1: WHOLE/SPLIT Recognition
- `scripts/01.generate_liheci_split_xfst.py` - Generates XFST rules for Stage 1
- `scripts/hfst_files/liheci_split.xfst` - Generated XFST file (663 states)
- `scripts/hfst_files/liheci_split.analyser.hfst` - Compiled HFST analyser
- `scripts/hfst_files/liheci_split.generator.hfst` - Compiled HFST generator
- `scripts/03.stage1_split_whole_recognition.py` - Stage 1 runner script

### Stage 2: REDUP Validation
- `scripts/02.generate_liheci_redup_xfst.py` - Generates XFST rules for REDUP patterns
- `scripts/hfst_files/liheci_redup.xfst` - Generated XFST file (225 states)
- `scripts/hfst_files/liheci_redup.analyser.hfst` - Compiled HFST analyser
- `scripts/hfst_files/liheci_redup.generator.hfst` - Compiled HFST generator
- `scripts/04.stage2_redup_recognition.py` - Stage 2 validator script

### Stage 3: Insertion Analysis
- `scripts/05.generate_insertion_context_xfst.py` - Generates XFST with replace rules for character annotation
- `scripts/hfst_files/liheci_insertion_annotator.xfst` - Generated XFST (43 states, 123 arcs)
- `scripts/hfst_files/liheci_insertion_annotator.hfst` - Compiled annotator
- `scripts/06.stage3_insertion_analysis.py` - Stage 3 insertion analyzer with Python extraction

### Output Files
- `outputs/liheci_hfst_outputs.tsv` - Stage 1→2 final output (206 rows)
- `outputs/liheci_hfst_run.log` - Stage 1 execution log
- `outputs/liheci_redup_validation.log` - Stage 2 validation log
- `outputs/liheci_insertion_analysis.tsv` - Stage 3 output with insertion analysis (206 rows)

## Valid REDUP Lemmas (18 total)

散步, 见面, 聊天, 睡觉, 把脉, 洗澡, 鼓掌, 敲门, 放假, 开会, 加班, 输液, 看病, 游泳, 排队, 散心, 请客, (1 more from latest run)

## Invalid REDUP Lemmas Filtered (45 total)

结婚, 离婚, 订婚, 分手, 放心, 担心, 灰心, 操心, 动心, 下课, 请假, 考试, 留学, 辞职, 生病, 住院, 鞠躬, 敬礼, 站岗, 受罪, 出院, 回家, 签名, 戒烟, 受伤, 扫兴, 接吻, 开枪, 受骗, 挨批, 干杯, 退休, 出事, 提醒, 出恭, 学习, 慷慨, 幽默, 滑稽, 军训, 体检, 同学, 告状, 请客

## Technical Details

### XFST Generation
Both Stage 1 and Stage 2 use the same technical approach:
1. Python script reads lexicon CSV
2. Generates XFST rules with proper formatting:
   - Chinese characters must be space-separated
   - Comments use `!` not `#`
   - Generator saved first, then inverted to analyser
3. Compile with `hfst-xfst -F` (OpenFST tropical semiring)

### HFST Lookup
- Command: `hfst-lookup <fst_file>`
- Input: Sentences (one per line)
- Output: TAB-separated format: `input\tanalysis\tweight`
- Parse analysis tags: `睡觉+Lemma+Verb-Object+SPLIT`
