#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
generate_liheci_split_xfst_v5_charlevel_safe.py

From liheci_lexicon.csv generate sentence-level HFST-XFST recognizer:
  - liheci_recognizer.xfst

Compile:
  hfst-xfst < liheci_recognizer.xfst

Outputs:
  liheci_split.generator.hfst   (tag -> sentence)
  liheci_split.analyser.hfst    (sentence -> tag)

Reads columns:
  Lemma, A, B, Type, RedupPattern, Transitivity, PronounInsertion, PPRequirement

Core idea:
  WHOLE  : head+tail contiguous
  SPLIT  : head ... tail with at least one "liheci hint" in between
           OR pronoun-insertion patterns (if allowed by PronounInsertion)
  REDUP  : AAB or A_XAB patterns

Refined SPLIT detection:
  - SPLIT must have >=1 InsertHintCore between head and tail
  - OR match pronoun insertion patterns
  - OR match EXT PP context patterns:
      EXT:COMITATIVE_PP(跟|和|与|同) etc.
"""

import csv
import re
from pathlib import Path
from typing import Dict, List, Tuple

INPUT_CSV = "data/liheci_lexicon.csv"
OUTPUT_XFST = "liheci_recognizer.xfst"

# Switch:
# - If True: add extra reason tags like +HINT/+PRONOBJ/+PRONPOSS/+EXT+COMITATIVE
# - If False: keep only WHOLE / SPLIT / SPLIT+REDUP
EMIT_REASON_TAGS = True


def map_type_tag(type_str: str) -> str:
    t = (type_str or "").strip()
    if not t:
        return "UnknownType"
    return t.replace(" ", "")


def chars_with_space(s: str) -> str:
    """Split into single characters separated by spaces, for XFST regex usage."""
    s = (s or "").strip()
    if not s:
        return ""
    return " ".join(list(s))


def xfst_seq(s: str) -> str:
    """Alias for chars_with_space, but name matches intention."""
    return chars_with_space(s)


def xfst_union_of_strings(items: List[str]) -> str:
    """
    Build an XFST union expression from strings, treating each string as char-level sequence.
    Example: ["跟","和","为了"] -> (跟 | 和 | 为 了)
    """
    seqs = [xfst_seq(x) for x in items if (x or "").strip()]
    seqs = [x for x in seqs if x]
    if not seqs:
        return "[]"
    if len(seqs) == 1:
        return seqs[0]
    return "(" + " | ".join(seqs) + ")"


def parse_redup_pattern(val: str) -> Tuple[bool, bool]:
    """
    Returns (has_AAB, has_A_XAB).
    CSV RedupPattern examples: "AAB", "A_XAB", "AAB;A_XAB", "".
    """
    v = (val or "").strip().upper()
    if not v:
        return (False, False)
    has_aab = "AAB" in v
    has_axab = "A_XAB" in v or "AXAB" in v
    return (has_aab, has_axab)


_EXT_PP_RE = re.compile(r"EXT:([A-Z_]+)\s*\(([^)]+)\)", re.IGNORECASE)


def parse_ext_pp(pp_req: str) -> List[Tuple[str, List[str]]]:
    """
    Parse PPRequirement field, extracting EXT specs:
      "EXT:COMITATIVE_PP(跟|和|与|同); EXT:GOAL_PP(给|对|向)"
    -> [("COMITATIVE_PP", ["跟","和","与","同"]), ("GOAL_PP", ["给","对","向"])]
    """
    s = (pp_req or "").strip()
    out: List[Tuple[str, List[str]]] = []
    for m in _EXT_PP_RE.finditer(s):
        pp_name = m.group(1).strip().upper()
        raw = m.group(2).strip()
        items = [x.strip() for x in raw.split("|") if x.strip()]
        if items:
            out.append((pp_name, items))
    return out


def main():
    input_path = Path(INPUT_CSV)
    if not input_path.exists():
        raise SystemExit(f"ERROR: cannot find {INPUT_CSV}")

    rows: List[Dict[str, str]] = []
    with input_path.open(encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            lemma = (row.get("Lemma") or "").strip()
            head = (row.get("A") or "").strip()
            tail = (row.get("B") or "").strip()
            if not lemma or not head or not tail:
                continue
            rows.append(row)

    lines: List[str] = []

    # =======================
    # 1) Global definitions
    # =======================
    lines.append("! Auto-generated by generate_liheci_split_xfst_v5_charlevel_safe.py")
    lines.append("! Sentence-level Liheci WHOLE / SPLIT / REDUP recognizer (char-level safe unions + EXT PP + pronoun insertion)")
    lines.append("")

    lines.append("! =======================")
    lines.append("! Global symbol classes")
    lines.append("! =======================")

    # Quote punctuation symbols to avoid syntax collisions.
    punct_syms = ["，", "。", "、", "！", "？", "：", "；", ",", ".", "!", "?", "…"]
    punct_union = " | ".join([f'"{p}"' for p in punct_syms])
    lines.append(f"define Punct   ({punct_union});")

    # Any symbol, then remove punct to form LegalIns
    lines.append("define AnyChar ?;")
    lines.append("define LegalIns (AnyChar - Punct);")
    lines.append("")

    # Mid-char for A_XAB redup
    lines.append("define RedupMid (一 | 了);")
    lines.append("")

    # -----------------------
    # Pronoun / possessive (char-level safe)
    # -----------------------
    lines.append("! Pronouns / possessive (char-level safe)")
    pronouns = [
        "我", "你", "他", "她", "它", "咱",
        "我们", "你们", "他们", "她们", "它们", "咱们",
        "大家", "人家", "谁", "自己",
    ]
    pron_union = xfst_union_of_strings(pronouns)
    lines.append(f"define Pronoun {pron_union};")
    lines.append("define PossMark 的;")
    lines.append("define PronPoss (Pronoun PossMark);")
    lines.append("")

    # -----------------------
    # InsertHintCore (sample-driven, char-level safe)
    # -----------------------
    lines.append("! -----------------------")
    lines.append("! Insert hints (keep compact; expand only if your data needs it)")
    lines.append("! -----------------------")

    lines.append("define Aspect   (了 | 过 | 着);")
    # from examples like 起不了床 => "不 了" as a sequence
    lines.append("define NegPot   (不 了);")

    # Numbers: Chinese numerals + Arabic digits
    numerals = ["一", "二", "两", "三", "四", "五", "六", "七", "八", "九", "十", "几", "百", "半",
                "0", "1", "2", "3", "4", "5", "6", "7", "8", "9"]
    lines.append(f"define Numeral  ({xfst_union_of_strings(numerals)});")

    demos = ["这", "那"]
    lines.append(f"define Demo     ({xfst_union_of_strings(demos)});")

    # Classifiers (mostly single-char; keep as union)
    cls = ["个", "次", "回", "遍", "场", "顿", "口", "句", "声", "趟", "阵", "通", "番", "把",
           "根", "瓶", "条", "只", "支", "本", "辆", "杯", "碗", "件", "双", "张", "种", "块",
           "匹", "头", "份", "班", "手", "针", "斤", "首", "节"]
    lines.append(f"define CL ({xfst_union_of_strings(cls)});")

    # NumCL:
    #   (Numeral)? CL
    #   Demo CL
    #   Demo (Numeral)? CL
    lines.append("define NumCL ( ((Numeral)?) CL | Demo CL | Demo ((Numeral)?) CL );")

    # Time units (char-level sequences):
    # 小时=小 时, 分钟=分 钟, 会儿=会 儿, etc.
    time_units = ["年", "月", "天", "小时", "分钟", "会儿"]
    lines.append(f"define TimeUnit2 ({xfst_union_of_strings(time_units)});")

    # NumTime examples:
    #   一 (个)? 小时 / 三 (个)? 月 / 半 个 月 / 半 天 / 一 整 天 / 一 辈 子
    lines.append("define NumTime ( Numeral ((个)?) TimeUnit2 | 半 个 月 | 半 天 | 一 整 天 | 一 辈 子 );")

    # Result complements as a weak hint (optional)
    rescomp = ["完", "好", "住", "开", "掉", "成", "到", "来", "去"]
    lines.append(f"define ResultComp ({xfst_union_of_strings(rescomp)});")

    # Degree-ish
    degree = ["下", "点"]
    lines.append(f"define Degree ({xfst_union_of_strings(degree)});")

    # DeMark
    lines.append("define DeMark 的;")

    # Core hints
    lines.append("define InsertHintCore (Aspect | NegPot | NumCL | NumTime | PronPoss | ResultComp | Degree | DeMark);")
    lines.append("")

    all_transducer_names: List[str] = []

    # =======================
    # 2) Per lemma patterns + transducers
    # =======================
    for idx, row in enumerate(rows, start=1):
        lemma = (row.get("Lemma") or "").strip()
        head = (row.get("A") or "").strip()
        tail = (row.get("B") or "").strip()
        type_str = (row.get("Type") or "").strip()

        redup_pat_raw = (row.get("RedupPattern") or "").strip()
        transitivity = (row.get("Transitivity") or "").strip()  # currently unused, reserved
        pron_ins = (row.get("PronounInsertion") or "").strip().upper()
        pp_req = (row.get("PPRequirement") or "").strip()

        type_tag = map_type_tag(type_str)
        head_chars = chars_with_space(head)
        tail_chars = chars_with_space(tail)
        if not head_chars or not tail_chars:
            continue

        base = f"L{idx:03d}"

        # -------------------
        # Upper tags
        # -------------------
        whole_upper = f"{lemma}+Lemma+{type_tag}+WHOLE"
        split_upper = f"{lemma}+Lemma+{type_tag}+SPLIT"
        redup_upper = f"{lemma}+Lemma+{type_tag}+SPLIT+REDUP"

        lines.append(f"! === {lemma}  (Type={type_str}, Trans={transitivity}, PronIns={pron_ins}, PPReq={pp_req}) ===")

        # -------------------
        # Core patterns (no surrounding ?*)
        # -------------------
        whole_core = f"{base}WholeCore"
        split_hint_core = f"{base}SplitHintCore"
        split_pron_obj_core = f"{base}SplitPronObjCore"
        split_pron_poss_core = f"{base}SplitPronPossCore"
        split_union_core = f"{base}SplitUnionCore"

        # WHOLE core: contiguous head+tail
        lines.append(f"define {whole_core} {head_chars} {tail_chars};")

        # SPLIT-hint core: must contain at least one InsertHintCore between head and tail
        lines.append(f"define {split_hint_core} {head_chars} (LegalIns)* InsertHintCore (LegalIns)* {tail_chars};")

        # Pronoun insertion cores controlled by PronounInsertion column
        emit_pron_obj = pron_ins in {"PRON_OBJ_OK", "PRON_POSS_PREFERRED", "PRON_POSS_REQUIRED"} or "PRON_OBJ_OK" in pron_ins
        emit_pron_poss = pron_ins in {"PRON_POSS_PREFERRED", "PRON_POSS_REQUIRED"} or "PRON_POSS" in pron_ins

        if emit_pron_obj:
            # Head + Pronoun + (optional stuff) + Tail
            lines.append(f"define {split_pron_obj_core} {head_chars} Pronoun (LegalIns)* {tail_chars};")
        else:
            lines.append(f"define {split_pron_obj_core} [];")

        if emit_pron_poss:
            # Head + Pronoun + 的 + (optional stuff) + Tail
            lines.append(f"define {split_pron_poss_core} {head_chars} Pronoun PossMark (LegalIns)* {tail_chars};")
        else:
            lines.append(f"define {split_pron_poss_core} [];")

        # Union of SPLIT cores
        lines.append(f"define {split_union_core} ({split_hint_core} | {split_pron_obj_core} | {split_pron_poss_core});")

        # -------------------
        # REDUP core (optional)
        # -------------------
        has_aab, has_axab = parse_redup_pattern(redup_pat_raw)
        redup_core = f"{base}RedupCore"
        if has_aab or has_axab:
            parts = []
            if has_aab:
                parts.append(f"{head_chars} {head_chars} {tail_chars}")
            if has_axab:
                parts.append(f"{head_chars} RedupMid {head_chars} {tail_chars}")
            lines.append(f"define {redup_core} (" + " | ".join(parts) + ");")
        else:
            lines.append(f"define {redup_core} [];")

        # -------------------
        # Sentence-level wrappers (+ optional exclude redup)
        # -------------------
        whole_pat = f"{base}WholePat"
        split_pat = f"{base}SplitPat"
        redup_pat_name = f"{base}RedupPat"

        lines.append(f"define {redup_pat_name} (AnyChar)* {redup_core} (AnyChar)*;")
        lines.append(f"define {whole_pat} (AnyChar)* {whole_core} (AnyChar)*;")
        lines.append(f"define {split_pat} (AnyChar)* {split_union_core} (AnyChar)*;")

        # Exclude redup from WHOLE/SPLIT if redup exists
        whole_only = f"{base}WholeOnly"
        split_only = f"{base}SplitOnly"
        if has_aab or has_axab:
            lines.append(f"define {whole_only} ({whole_pat} - {redup_pat_name});")
            lines.append(f"define {split_only} ({split_pat} - {redup_pat_name});")
        else:
            lines.append(f"define {whole_only} {whole_pat};")
            lines.append(f"define {split_only} {split_pat};")

        # -------------------
        # EXT PP context patterns (coarse, but useful)
        # EXT:COMITATIVE_PP(跟|和|与|同) etc.
        # Assume PP is near-left of the liheci occurrence:
        #   AnyChar* Prep NPish (WHOLEcore or SPLITcore) AnyChar*
        # NPish chunk between PP and liheci: require >=1 LegalIns
        # -------------------
        ext_specs = parse_ext_pp(pp_req)
        ext_pat_names: List[Tuple[str, str]] = []  # (pp_name_norm, pat_name)

        for k, (pp_name, preps) in enumerate(ext_specs, start=1):
            prep_set_name = f"{base}ExtPrep{k}"
            ext_pat = f"{base}ExtPat{k}"

            norm = pp_name.upper()
            norm = norm.replace("_PP", "")
            norm = norm.replace("PP", "")

            prep_union = xfst_union_of_strings(preps)
            lines.append(f"define {prep_set_name} {prep_union};")

            lines.append(
                f"define {ext_pat} (AnyChar)* {prep_set_name} LegalIns (LegalIns)* ({whole_core} | {split_union_core}) (AnyChar)*;"
            )
            ext_pat_names.append((norm, ext_pat))

        # -------------------
        # Transducers
        # -------------------
        tr_names_for_this: List[str] = []

        whole_tr = f"{base}Whole"
        split_tr = f"{base}Split"
        lines.append(f'define {whole_tr} "{whole_upper}" : {whole_only};')
        lines.append(f'define {split_tr} "{split_upper}" : {split_only};')
        tr_names_for_this.extend([whole_tr, split_tr])

        if has_aab or has_axab:
            redup_tr = f"{base}Redup"
            lines.append(f'define {redup_tr} "{redup_upper}" : {redup_pat_name};')
            tr_names_for_this.append(redup_tr)

        # -------------------
        # Optional: reason tags (debugging / analysis)
        # -------------------
        if EMIT_REASON_TAGS:
            hint_tr = f"{base}SplitHint"
            lines.append(f'define {hint_tr} "{lemma}+Lemma+{type_tag}+SPLIT+HINT" : ((AnyChar)* {split_hint_core} (AnyChar)*);')
            tr_names_for_this.append(hint_tr)

            if emit_pron_obj:
                pronobj_tr = f"{base}SplitPronObj"
                lines.append(f'define {pronobj_tr} "{lemma}+Lemma+{type_tag}+SPLIT+PRONOBJ" : ((AnyChar)* {split_pron_obj_core} (AnyChar)*);')
                tr_names_for_this.append(pronobj_tr)

            if emit_pron_poss:
                pronposs_tr = f"{base}SplitPronPoss"
                lines.append(f'define {pronposs_tr} "{lemma}+Lemma+{type_tag}+SPLIT+PRONPOSS" : ((AnyChar)* {split_pron_poss_core} (AnyChar)*);')
                tr_names_for_this.append(pronposs_tr)

            for norm, pat_name in ext_pat_names:
                ext_tr = f"{base}Ext{norm}"
                lines.append(f'define {ext_tr} "{lemma}+Lemma+{type_tag}+EXT+{norm}" : {pat_name};')
                tr_names_for_this.append(ext_tr)

        lines.append("")
        all_transducer_names.extend(tr_names_for_this)

    # =======================
    # 3) Union + save
    # =======================
    lines.append("! =======================")
    lines.append("! Union all lemma transducers (grouped to avoid state explosion)")
    lines.append("! =======================")
    
    # Split transducers into chunks and create intermediate groups
    # This prevents HFST from exploding during compilation
    # Instead of: regex L001Whole | L001Split | ... | L131SplitHint ;
    # We use:
    #   define group1 = L001Whole | ... | L030SplitHint ;
    #   define group2 = L031Whole | ... | L060SplitHint ;
    #   define group3 = L061Whole | ... | L131SplitHint ;
    #   regex group1 | group2 | group3 ;
    
    chunk_size = 30  # Compile ~30 transducers per group
    groups = []
    
    if all_transducer_names:
        for i in range(0, len(all_transducer_names), chunk_size):
            chunk = all_transducer_names[i:i+chunk_size]
            group_name = f"group{i//chunk_size + 1}"
            lines.append(f"define {group_name} " + " | ".join(chunk) + " ;")
            groups.append(group_name)
    
    if groups:
        lines.append("regex " + " | ".join(groups) + " ;")
    else:
        lines.append("regex [] ;  ! empty language")
    
    lines.append("")
    lines.append("! Stack top is generator: Upper=tags, Lower=sentences")
    lines.append("save stack liheci_split.generator.hfst")
    lines.append("")
    lines.append("invert net")
    lines.append("save stack liheci_split.analyser.hfst")
    lines.append("")
    lines.append("quit")
    lines.append("")

    Path(OUTPUT_XFST).write_text("\n".join(lines), encoding="utf-8")
    print(f"[OK] generated {OUTPUT_XFST} with {len(rows)} lemmas; reason_tags={EMIT_REASON_TAGS}")
    print(f"[INFO] Using {len(groups)} groups of ~{chunk_size} transducers each to avoid state explosion")


if __name__ == "__main__":
    main()
