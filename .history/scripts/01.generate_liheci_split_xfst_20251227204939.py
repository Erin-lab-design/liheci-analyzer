#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
generate_liheci_split_xfst_v3.py

从 data/liheci_lexicon.csv 自动生成：data/liheci_split.xfst

编译：
    hfst-xfst < data/liheci_split.xfst

生成：
    data/liheci_split.generator.hfst   (upper: lemma-tag → lower: 句子模式)
    data/liheci_split.analyser.hfst    (upper: 句子 → lower: lemma-tag)

输出标签示例：
    散心+Lemma+Verb-Object+WHOLE
    散心+Lemma+Verb-Object+SPLIT
    散心+Lemma+Verb-Object+SPLIT+REDUP

关键约束：
- SPLIT 插入语中：允许任意长度，但 **不允许任何标点符号**
- 一旦出现标点，就视为断开（后续你要做句法分析再处理）
"""

import csv
from pathlib import Path

INPUT_CSV = "data/liheci_lexicon.csv"
OUTPUT_XFST = "data/liheci_split.xfst"

OUTPUT_GENERATOR = "data/liheci_split.generator.hfst"
OUTPUT_ANALYSER = "data/liheci_split.analyser.hfst"

# 把 union 拆块，避免 regex 一行超长 & 也避免你之前那种“多行 regex 被当成多条命令”
UNION_CHUNK_SIZE = 40


def map_type_tag(type_str: str) -> str:
    """
    把 CSV 里的 Type 映射到 HFST 用的类型标签（无空格）。
    例：Verb-Object, Pseudo V-O, Modifier-Head, Simplex Word
    """
    t = (type_str or "").strip()
    if not t:
        return "UnknownType"
    return t.replace(" ", "")


def has_redup(notes: str) -> bool:
    """
    Notes 里含 'AAB' 就当作这个 lemma 支持重叠形式
    （比如 散散心(AAB)，握了一下手(AAB) 等）
    """
    return bool(notes) and ("AAB" in notes)


def chars_with_space(s: str) -> str:
    """把字符串拆成单字符并用空格隔开，供 xfst 正则使用。"""
    s = (s or "").strip()
    if not s:
        return ""
    return " ".join(list(s))


def chunked(seq, n):
    for i in range(0, len(seq), n):
        yield seq[i:i + n]


def main():
    input_path = Path(INPUT_CSV)
    if not input_path.exists():
        raise SystemExit(f"ERROR: 找不到 {INPUT_CSV}，请确认文件名和路径。")

    rows = []
    with input_path.open(encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            lemma = (row.get("Lemma") or "").strip()
            head = (row.get("A") or "").strip()
            tail = (row.get("B") or "").strip()

            # 跳过残缺行
            if not lemma or not head or not tail:
                continue

            rows.append(row)

    lines: list[str] = []

    # =========================================================
    # 1) 公共定义
    # =========================================================
    # 标点列表尽量覆盖常见的中英文标点（你说“所有标点都不行”，那就尽量全）
    punct_items = [
        "，", "。", "、", "！", "？", "：", "；", "…", "—", "·",
        ",", ".", "!", "?", ":", ";",
        "“", "”", "‘", "’", "\"", "'",
        "（", "）", "(", ")",
        "【", "】", "[", "]",
        "《", "》", "<", ">",
        "「", "」", "『", "』",
        "～", "~", "…",
        "·", "•",
        "—", "-", "－", "–",
        "／", "/", "\\",
        "＠", "@", "＃", "#", "＆", "&",
        "％", "%", "＋", "+", "＝", "=",
        "＊", "*", "＾", "^",
        "｜", "|",
    ]
    punct_def = 'define Punct [ ' + " | ".join(f'"{p}"' for p in punct_items) + " ] ;"

    lines.extend([
        "! Auto-generated by generate_liheci_split_xfst_v3.py",
        "! Sentence-level Liheci WHOLE / SPLIT / REDUP recognizer",
        "",
        "! 1) 符号集合",
        punct_def,
        "define AnyChar [?] ;",
        "define LegalIns [ AnyChar - Punct ] ;",
        'define RedupMid [ "一" | "了" ] ;',
        "",
    ])

    all_transducer_names: list[str] = []

    # =========================================================
    # 2) 每个 lemma 生成 pattern + transducer
    # =========================================================
    for idx, row in enumerate(rows, start=1):
        lemma = (row.get("Lemma") or "").strip()
        head = (row.get("A") or "").strip()
        tail = (row.get("B") or "").strip()
        type_str = (row.get("Type") or "").strip()
        notes = (row.get("Notes") or "").strip()

        type_tag = map_type_tag(type_str)

        head_chars = chars_with_space(head)
        tail_chars = chars_with_space(tail)
        if not head_chars or not tail_chars:
            continue

        base = f"L{idx:03d}"  # L001, L002, ...

        whole_upper = f"{lemma}+Lemma+{type_tag}+WHOLE"
        split_upper = f"{lemma}+Lemma+{type_tag}+SPLIT"
        redup_upper = f"{lemma}+Lemma+{type_tag}+SPLIT+REDUP"

        whole_pat_name = f"{base}WholePat"
        split_pat_name = f"{base}SplitPat"

        whole_only_pat_name = f"{base}WholeOnlyPat"
        split_only_pat_name = f"{base}SplitOnlyPat"

        whole_tr_name = f"{base}Whole"
        split_tr_name = f"{base}Split"

        lines.append(f"! === {lemma} (Type={type_str}) ===")

        # WHOLE：句子中出现连续的 head+tail（中间不允许插入）
        lines.append(f"define {whole_pat_name} ?* {head_chars} {tail_chars} ?* ;")

        # SPLIT：head 后至少插入 1 个 LegalIns（可很长），中间禁止任何标点（Punct）
        lines.append(f"define {split_pat_name} ?* {head_chars} LegalIns LegalIns* {tail_chars} ?* ;")

        transducers_for_this: list[str] = []

        if has_redup(notes):
            redup_pat_name = f"{base}RedupPat"
            redup_tr_name = f"{base}Redup"

            # 重叠：AAB 或 A_XAB（X ∈ {一, 了}）
            lines.append(
                f"define {redup_pat_name} "
                f"( ?* {head_chars} {head_chars} {tail_chars} ?* "
                f"| ?* {head_chars} RedupMid {head_chars} {tail_chars} ?* ) ;"
            )

            # WHOLE / SPLIT 排除重叠句
            lines.append(f"define {whole_only_pat_name} {whole_pat_name} - {redup_pat_name} ;")
            lines.append(f"define {split_only_pat_name} {split_pat_name} - {redup_pat_name} ;")

            # transducer：upper 为 tag，lower 为句子模式
            lines.append(f'define {whole_tr_name} "{whole_upper}" : {whole_only_pat_name} ;')
            lines.append(f'define {split_tr_name} "{split_upper}" : {split_only_pat_name} ;')
            lines.append(f'define {redup_tr_name} "{redup_upper}" : {redup_pat_name} ;')

            transducers_for_this.extend([whole_tr_name, split_tr_name, redup_tr_name])
        else:
            lines.append(f"define {whole_only_pat_name} {whole_pat_name} ;")
            lines.append(f"define {split_only_pat_name} {split_pat_name} ;")

            lines.append(f'define {whole_tr_name} "{whole_upper}" : {whole_only_pat_name} ;')
            lines.append(f'define {split_tr_name} "{split_upper}" : {split_only_pat_name} ;')

            transducers_for_this.extend([whole_tr_name, split_tr_name])

        lines.append("")
        all_transducer_names.extend(transducers_for_this)

    # =========================================================
    # 3) union + 保存（关键修复点）
    # =========================================================
    lines.append("! 组合所有 lemma 的 transducer（chunk union，避免多行 regex 被当命令）")

    if not all_transducer_names:
        lines.append("regex [] ; ! 没有 lemma，空语言")
    else:
        # 先把 transducer 名字按块 define 成 U001/U002/...
        u_names: list[str] = []
        for k, chunk in enumerate(chunked(all_transducer_names, UNION_CHUNK_SIZE), start=1):
            u = f"U{k:03d}"
            u_names.append(u)
            lines.append(f"define {u} " + " | ".join(chunk) + " ;")

        # 再 define ALL
        lines.append("define ALL " + " | ".join(u_names) + " ;")
        lines.append("")

        # regex 必须是一条完整命令（同一行最稳）
        lines.append("regex ALL ;")

    lines.extend([
        "",
        "! 现在栈顶是 generator：",
        "! 上层：<Lemma>+Lemma+TypeTag+FORM(+REDUP?)",
        "! 下层：满足对应模式的整句",
        f"save stack {OUTPUT_GENERATOR}",
        "",
        "invert net",
        f"save stack {OUTPUT_ANALYSER}",
        "",
        "quit",
        "",
    ])

    Path(OUTPUT_XFST).write_text("\n".join(lines), encoding="utf-8")
    print(f"[OK] 已生成 {OUTPUT_XFST}，共 {len(rows)} 个 lemma")

if __name__ == "__main__":
    main()
