#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
generate_liheci_split_xfst_v3.py

从 liheci_lexicon.csv 自动生成：liheci_split.xfst

编译：
    hfst-xfst < liheci_split.xfst

生成：
    liheci_split.generator.hfst   (lemma-tag → 句子)
    liheci_split.analyser.hfst    (句子 → lemma-tag)

输出标签示例：
    散心+Lemma+Verb-Object+WHOLE
    散心+Lemma+Verb-Object+SPLIT
    散心+Lemma+Verb-Object+SPLIT+REDUP
"""

import csv
from pathlib import Path

INPUT_CSV = "data/liheci_lexicon.csv"
OUTPUT_XFST = "data/liheci_split.xfst"


def map_type_tag(type_str: str) -> str:
    """
    把 CSV 里的 Type 映射到 HFST 用的类型标签（无空格）。
    例：Verb-Object, Pseudo V-O, Modifier-Head, Simplex Word
    """
    t = (type_str or "").strip()
    if not t:
        return "UnknownType"
    return t.replace(" ", "")


def has_redup(notes: str) -> bool:
    """
    Notes 里含 'AAB' 就当作这个 lemma 支持重叠形式
    （比如 散散心(AAB)，握了一下手(AAB) 等）
    """
    return bool(notes) and ("AAB" in notes)


def chars_with_space(s: str) -> str:
    """把字符串拆成单字符并用空格隔开，供 xfst 正则使用。"""
    s = (s or "").strip()
    if not s:
        return ""
    return " ".join(list(s))


def main():
    input_path = Path(INPUT_CSV)
    if not input_path.exists():
        raise SystemExit(f"ERROR: 找不到 {INPUT_CSV}，请确认文件名和路径。")

    rows = []
    with input_path.open(encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            lemma = (row.get("Lemma") or "").strip()
            head = (row.get("A") or "").strip()
            tail = (row.get("B") or "").strip()

            # 跳过残缺行（比如单独一行 'c'）
            if not lemma or not head or not tail:
                continue

            rows.append(row)

    lines: list[str] = []

    # =========================================================
    # 1) 公共定义
    # =========================================================
    lines.extend([
        "! Auto-generated by generate_liheci_split_xfst_v3.py",
        "! Sentence-level Liheci WHOLE / SPLIT / REDUP recognizer",
        "",
        "! 1) 符号集合",
        # NOTE: xfst 的字符类/字面符号写法更稳：把标点做成一个字符类
        # 这里同时收中文全角 + 英文半角；你要加更多标点就继续往里加。
        'define Punct [ "，" | "。" | "、" | "！" | "？" | "：" | "；" | "," | "." | "!" | "?" | ":" | ";" ] ;',
        # AnyChar：sigma 中任意符号（前提：编译时字母表里确实出现过这些符号）
        "define AnyChar [?] ;",
        # LegalIns：插入语可用字符 = 任意字符 - 标点
        # 你之前要求“所有标点符号都不行”，这里就是严格禁止 Punct。
        "define LegalIns [ AnyChar - Punct ] ;",
        # A_XAB 中间的 X（如果你以后要支持“着/过”等也可以加）
        'define RedupMid [ "一" | "了" ] ;',
        "",
    ])

    all_transducer_names: list[str] = []

    # =========================================================
    # 2) 每个 lemma 生成 pattern + transducer
    # =========================================================
    for idx, row in enumerate(rows, start=1):
        lemma = (row.get("Lemma") or "").strip()
        head = (row.get("A") or "").strip()
        tail = (row.get("B") or "").strip()
        type_str = (row.get("Type") or "").strip()
        notes = (row.get("Notes") or "").strip()

        type_tag = map_type_tag(type_str)

        head_chars = chars_with_space(head)
        tail_chars = chars_with_space(tail)
        if not head_chars or not tail_chars:
            continue

        base = f"L{idx:03d}"  # L001, L002, ...

        whole_upper = f"{lemma}+Lemma+{type_tag}+WHOLE"
        split_upper = f"{lemma}+Lemma+{type_tag}+SPLIT"
        redup_upper = f"{lemma}+Lemma+{type_tag}+SPLIT+REDUP"

        whole_pat_name = f"{base}WholePat"
        split_pat_name = f"{base}SplitPat"

        whole_only_pat_name = f"{base}WholeOnlyPat"
        split_only_pat_name = f"{base}SplitOnlyPat"

        whole_tr_name = f"{base}Whole"
        split_tr_name = f"{base}Split"

        lines.append(f"! === {lemma} (Type={type_str}) ===")

        # --- 基本 WHOLE / SPLIT 模式 ---
        # WHOLE：句子中出现连续的 head+tail（中间不允许插入）
        lines.append(f"define {whole_pat_name} ?* {head_chars} {tail_chars} ?* ;")

        # SPLIT：head 后至少插入 1 个 LegalIns（且中间不能出现任何标点）
        # 解释：LegalIns LegalIns* = 至少一个字符的插入语
        lines.append(f"define {split_pat_name} ?* {head_chars} LegalIns LegalIns* {tail_chars} ?* ;")

        transducers_for_this: list[str] = []

        if has_redup(notes):
            # 重叠模式：
            # - AAB: H H T
            # - A_XAB: H RedupMid H T (X ∈ {一, 了})
            redup_pat_name = f"{base}RedupPat"
            redup_tr_name = f"{base}Redup"

            lines.append(
                f"define {redup_pat_name} "
                f"( ?* {head_chars} {head_chars} {tail_chars} ?* "
                f"| ?* {head_chars} RedupMid {head_chars} {tail_chars} ?* ) ;"
            )

            # WHOLE / SPLIT 排除重叠句
            lines.append(f"define {whole_only_pat_name} {whole_pat_name} - {redup_pat_name} ;")
            lines.append(f"define {split_only_pat_name} {split_pat_name} - {redup_pat_name} ;")

            # 绑定 transducer（upper: tag，lower: 句子模式）
            lines.append(f'define {whole_tr_name} "{whole_upper}" : {whole_only_pat_name} ;')
            lines.append(f'define {split_tr_name} "{split_upper}" : {split_only_pat_name} ;')
            lines.append(f'define {redup_tr_name} "{redup_upper}" : {redup_pat_name} ;')

            transducers_for_this.extend([whole_tr_name, split_tr_name, redup_tr_name])

        else:
            # 无重叠：直接用 WholePat / SplitPat
            lines.append(f"define {whole_only_pat_name} {whole_pat_name} ;")
            lines.append(f"define {split_only_pat_name} {split_pat_name} ;")

            lines.append(f'define {whole_tr_name} "{whole_upper}" : {whole_only_pat_name} ;')
            lines.append(f'define {split_tr_name} "{split_upper}" : {split_only_pat_name} ;')

            transducers_for_this.extend([whole_tr_name, split_tr_name])

        lines.append("")
        all_transducer_names.extend(transducers_for_this)

    # =========================================================
    # 3) union + 保存
    # =========================================================
    lines.extend([
        "! 组合所有 lemma 的 transducer",
    ])

    if all_transducer_names:
        # regex 那行很长会影响可读性，这里拆成多行拼出来（xfst 里仍然是 1 条 regex）
        lines.append("regex")
        for i, name in enumerate(all_transducer_names):
            bar = " |" if i < len(all_transducer_names) - 1 else ""
            lines.append(f"  {name}{bar}")
        lines.append(";")
    else:
        lines.append("regex [] ; ! 没有 lemma，空语言")

    lines.extend([
        "",
        "! 现在栈顶是 generator：",
        "! 上层：<Lemma>+Lemma+TypeTag+FORM(+REDUP?)",
        "! 下层：满足对应模式的整句",
        "save stack liheci_split.generator.hfst",
        "",
        "invert net",
        "save stack liheci_split.analyser.hfst",
        "",
        "quit",
        "",
    ])

    Path(OUTPUT_XFST).write_text("\n".join(lines), encoding="utf-8")
    print(f"[OK] 已生成 {OUTPUT_XFST}，共 {len(rows)} 个 lemma")


if __name__ == "__main__":
    main()
