#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
generate_liheci_split_xfst_v4_integrated.py

From liheci_lexicon.csv generate sentence-level HFST-XFST recognizer:
  - liheci_split.xfst

Compile:
  hfst-xfst < liheci_split.xfst

Outputs:
  liheci_split.generator.hfst   (tag -> sentence)
  liheci_split.analyser.hfst    (sentence -> tag)

This version reads new columns:
  RedupPattern, Transitivity, PronounInsertion, PPRequirement

Refined SPLIT detection:
  SPLIT must have at least one "liheci hint" between head and tail
  OR match pronoun-insertion patterns
  OR match EXT PP context patterns (e.g., 跟/和/与/同 + NP + 见面)
"""

import csv
import re
from pathlib import Path
from typing import Dict, List, Tuple

INPUT_CSV = "liheci_lexicon.csv"
OUTPUT_XFST = "liheci_split.xfst"

# Switch:
# - If True: add extra reason tags like +HINT/+PRONOBJ/+PRONPOSS/+EXT+COMITATIVE
# - If False: keep only WHOLE / SPLIT / SPLIT+REDUP
EMIT_REASON_TAGS = True


def map_type_tag(type_str: str) -> str:
    t = (type_str or "").strip()
    if not t:
        return "UnknownType"
    return t.replace(" ", "")


def chars_with_space(s: str) -> str:
    """Split into single characters separated by spaces, for XFST regex usage."""
    s = (s or "").strip()
    if not s:
        return ""
    return " ".join(list(s))


def parse_redup_pattern(val: str) -> Tuple[bool, bool]:
    """
    Returns (has_AAB, has_A_XAB).
    CSV RedupPattern examples: "AAB", "A_XAB", "AAB;A_XAB", "".
    """
    v = (val or "").strip().upper()
    if not v:
        return (False, False)
    has_aab = "AAB" in v
    has_axab = "A_XAB" in v or "AXAB" in v
    return (has_aab, has_axab)


_EXT_PP_RE = re.compile(r"EXT:([A-Z_]+)\s*\(([^)]+)\)", re.IGNORECASE)


def parse_ext_pp(pp_req: str) -> List[Tuple[str, List[str]]]:
    """
    Parse PPRequirement field, extracting EXT specs:
      "EXT:COMITATIVE_PP(跟|和|与|同); EXT:GOAL_PP(给|对|向)"
    -> [("COMITATIVE_PP", ["跟","和","与","同"]), ("GOAL_PP", ["给","对","向"])]
    """
    s = (pp_req or "").strip()
    out: List[Tuple[str, List[str]]] = []
    for m in _EXT_PP_RE.finditer(s):
        pp_name = m.group(1).strip().upper()
        raw = m.group(2).strip()
        items = [x.strip() for x in raw.split("|") if x.strip()]
        if items:
            out.append((pp_name, items))
    return out


def main():
    input_path = Path(INPUT_CSV)
    if not input_path.exists():
        raise SystemExit(f"ERROR: cannot find {INPUT_CSV}")

    rows: List[Dict[str, str]] = []
    with input_path.open(encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            lemma = (row.get("Lemma") or "").strip()
            head = (row.get("A") or "").strip()
            tail = (row.get("B") or "").strip()
            if not lemma or not head or not tail:
                continue
            rows.append(row)

    lines: List[str] = []

    # =======================
    # 1) Global definitions
    # =======================
    lines.append("! Auto-generated by generate_liheci_split_xfst_v4_integrated.py")
    lines.append("! Sentence-level Liheci WHOLE / SPLIT / REDUP recognizer (refined SPLIT hints + EXT PP + pronoun insertion)")
    lines.append("")
    lines.append("! =======================")
    lines.append("! Global symbol classes")
    lines.append("! =======================")
    lines.append('define Punct   [， | 。 | 、 | ！ | ？ | ： | ； | "，" | "。" | "," | "." | "!" | "?" | "…" | "、"];')
    lines.append("define AnyChar [?];")
    lines.append("define LegalIns [AnyChar - Punct];")
    lines.append("")

    # Mid-char for A_XAB redup
    lines.append("define RedupMid [一 | 了];")
    lines.append("")

    # -----------------------
    # Pronoun / possessive
    # -----------------------
    lines.append("! Pronouns / possessive")
    lines.append("define Pronoun [我|你|他|她|它|咱|我们|你们|他们|她们|它们|咱们|大家|人家|谁|自己];")
    lines.append("define PossMark [的];")
    lines.append("define PronPoss [Pronoun PossMark];")
    lines.append("")

    # -----------------------
    # InsertHintCore (sample-driven, char-level safe)
    # -----------------------
    lines.append("! -----------------------")
    lines.append("! Insert hints (keep compact; expand only if your data needs it)")
    lines.append("! -----------------------")
    lines.append("define Aspect   [了|过|着];")
    lines.append("define NegPot   [不 了];")  # from examples like 起不了床 (char-level sequence)

    # Numbers: Chinese numerals + Arabic digits (optional)
    lines.append("define Numeral  [一|二|两|三|四|五|六|七|八|九|十|几|百|半|0|1|2|3|4|5|6|7|8|9];")
    lines.append("define Demo     [这|那];")

    # Classifiers: focus on those that actually appear often in liheci splits
    # (You can add more later; char-level only, so avoid multi-char items here)
    lines.append("define CL [个|次|回|遍|场|顿|口|句|声|趟|阵|通|番|把|根|瓶|条|只|支|本|辆|杯|碗|件|双|张|种|块|匹|头|份|班|手|针|斤|首|节];")

    # Numeral may be omitted when effectively 'one': 见个面 / 洗个澡 / 看个病
    lines.append("define NumCL [(Numeral)? CL] | [Demo CL] | [Demo (Numeral)? CL];")

    # Time units (char-level sequences):
    # 小时=小 时, 分钟=分 钟, 会儿=会 儿, 半个月=半 个 月, 半天=半 天, 一辈子=一 辈 子, 一整天=一 整 天
    lines.append("define TimeUnit2 [年 | 月 | 天 | 小 时 | 分 钟 | 会 儿];")
    lines.append("define NumTime [Numeral (个)? TimeUnit2] | [半 个 月] | [半 天] | [一 整 天] | [一 辈 子] ;")

    # Result complements as a weak hint (optional)
    lines.append("define ResultComp [完|好|住|开|掉|成|到|来|去];")

    # 'Degree' often overlaps with measure-ish things; keep tiny
    lines.append("define Degree [下|点];")

    # A very weak marker seen inside many NPs: 的
    lines.append("define DeMark [的];")

    # Core hints: (aspect/neg-pot) OR (num+classifier / classifier) OR (num+time) OR (pron+的) OR (result/degree/的)
    lines.append("define InsertHintCore [Aspect | NegPot | NumCL | NumTime | PronPoss | ResultComp | Degree | DeMark];")
    lines.append("")

    all_transducer_names: List[str] = []

    # =======================
    # 2) Per lemma patterns + transducers
    # =======================
    for idx, row in enumerate(rows, start=1):
        lemma = (row.get("Lemma") or "").strip()
        head = (row.get("A") or "").strip()
        tail = (row.get("B") or "").strip()
        type_str = (row.get("Type") or "").strip()

        redup_pat = (row.get("RedupPattern") or "").strip()
        transitivity = (row.get("Transitivity") or "").strip()  # currently unused, kept for future
        pron_ins = (row.get("PronounInsertion") or "").strip().upper()
        pp_req = (row.get("PPRequirement") or "").strip()

        type_tag = map_type_tag(type_str)
        head_chars = chars_with_space(head)
        tail_chars = chars_with_space(tail)
        if not head_chars or not tail_chars:
            continue

        base = f"L{idx:03d}"

        # -------------------
        # Upper tags
        # -------------------
        whole_upper = f"{lemma}+Lemma+{type_tag}+WHOLE"
        split_upper = f"{lemma}+Lemma+{type_tag}+SPLIT"
        redup_upper = f"{lemma}+Lemma+{type_tag}+SPLIT+REDUP"

        lines.append(f"! === {lemma}  (Type={type_str}, Trans={transitivity}, PronIns={pron_ins}, PPReq={pp_req}) ===")

        # -------------------
        # Core patterns (no surrounding ?*)
        # -------------------
        whole_core = f"{base}WholeCore"
        split_hint_core = f"{base}SplitHintCore"
        split_pron_obj_core = f"{base}SplitPronObjCore"
        split_pron_poss_core = f"{base}SplitPronPossCore"
        split_union_core = f"{base}SplitUnionCore"

        # WHOLE core: contiguous head+tail
        lines.append(f"define {whole_core} {head_chars} {tail_chars} ;")

        # SPLIT-hint core: must contain at least one InsertHintCore between head and tail
        lines.append(f"define {split_hint_core} {head_chars} LegalIns* InsertHintCore LegalIns* {tail_chars} ;")

        # Pronoun insertion cores are optional depending on pron_ins column
        emit_pron_obj = pron_ins in {"PRON_OBJ_OK", "PRON_POSS_PREFERRED", "PRON_POSS_REQUIRED"} or "PRON_OBJ_OK" in pron_ins
        emit_pron_poss = pron_ins in {"PRON_POSS_PREFERRED", "PRON_POSS_REQUIRED"} or "PRON_POSS" in pron_ins

        if emit_pron_obj:
            # Head + Pronoun + (optional stuff) + Tail
            lines.append(f"define {split_pron_obj_core} {head_chars} Pronoun LegalIns* {tail_chars} ;")
        else:
            lines.append(f"define {split_pron_obj_core} [] ;")

        if emit_pron_poss:
            # Head + Pronoun + 的 + (optional stuff) + Tail
            lines.append(f"define {split_pron_poss_core} {head_chars} Pronoun PossMark LegalIns* {tail_chars} ;")
        else:
            lines.append(f"define {split_pron_poss_core} [] ;")

        # Union of SPLIT cores
        lines.append(f"define {split_union_core} ({split_hint_core} | {split_pron_obj_core} | {split_pron_poss_core}) ;")

        # -------------------
        # REDUP core (optional)
        # -------------------
        has_aab, has_axab = parse_redup_pattern(redup_pat)
        redup_core = f"{base}RedupCore"
        if has_aab or has_axab:
            parts = []
            if has_aab:
                parts.append(f"{head_chars} {head_chars} {tail_chars}")
            if has_axab:
                parts.append(f"{head_chars} RedupMid {head_chars} {tail_chars}")
            lines.append(f"define {redup_core} (" + " | ".join(parts) + ") ;")
        else:
            lines.append(f"define {redup_core} [] ;")

        # -------------------
        # Sentence-level wrappers (+ optional exclude redup)
        # -------------------
        whole_pat = f"{base}WholePat"
        split_pat = f"{base}SplitPat"
        redup_pat_name = f"{base}RedupPat"

        # sentence-level patterns
        lines.append(f"define {redup_pat_name} ?* {redup_core} ?* ;")
        lines.append(f"define {whole_pat} ?* {whole_core} ?* ;")
        lines.append(f"define {split_pat} ?* {split_union_core} ?* ;")

        # Exclude redup from WHOLE/SPLIT if redup exists
        whole_only = f"{base}WholeOnly"
        split_only = f"{base}SplitOnly"
        if has_aab or has_axab:
            lines.append(f"define {whole_only} {whole_pat} - {redup_pat_name} ;")
            lines.append(f"define {split_only} {split_pat} - {redup_pat_name} ;")
        else:
            lines.append(f"define {whole_only} {whole_pat} ;")
            lines.append(f"define {split_only} {split_pat} ;")

        # -------------------
        # EXT PP context patterns (coarse, but useful)
        # EXT:COMITATIVE_PP(跟|和|与|同) etc.
        # We assume PP is near-left of the liheci occurrence:
        #   ?* Prep NPish (WHOLEcore or SPLITcore) ?*
        # -------------------
        ext_specs = parse_ext_pp(pp_req)
        ext_pat_names: List[Tuple[str, str]] = []  # (pp_name_norm, pat_name)

        for k, (pp_name, preps) in enumerate(ext_specs, start=1):
            prep_set_name = f"{base}ExtPrep{k}"
            ext_pat = f"{base}ExtPat{k}"

            # Normalize PP name for tags
            norm = pp_name.upper()
            norm = norm.replace("_PP", "")
            norm = norm.replace("PP", "")

            # Define preposition set: these should be single chars in most cases
            prep_union = "|".join(preps)
            lines.append(f"define {prep_set_name} [{prep_union}];")

            # NP-ish chunk between PP and liheci: require at least 1 non-punct char
            # then allow either WHOLEcore or SPLITcore (core, no ?*)
            lines.append(
                f"define {ext_pat} ?* {prep_set_name} LegalIns LegalIns* ({whole_core} | {split_union_core}) ?* ;"
            )

            ext_pat_names.append((norm, ext_pat))

        # -------------------
        # Transducers
        # -------------------
        tr_names_for_this: List[str] = []

        whole_tr = f"{base}Whole"
        split_tr = f"{base}Split"
        lines.append(f'define {whole_tr} "{whole_upper}" : {whole_only} ;')
        lines.append(f'define {split_tr} "{split_upper}" : {split_only} ;')
        tr_names_for_this.extend([whole_tr, split_tr])

        if has_aab or has_axab:
            redup_tr = f"{base}Redup"
            lines.append(f'define {redup_tr} "{redup_upper}" : {redup_pat_name} ;')
            tr_names_for_this.append(redup_tr)

        # -------------------
        # Optional: reason tags (debugging / analysis)
        # -------------------
        if EMIT_REASON_TAGS:
            # +HINT (only the hint-core pattern)
            hint_tr = f"{base}SplitHint"
            lines.append(f'define {hint_tr} "{lemma}+Lemma+{type_tag}+SPLIT+HINT" : (?* {split_hint_core} ?*) ;')
            tr_names_for_this.append(hint_tr)

            if emit_pron_obj:
                pronobj_tr = f"{base}SplitPronObj"
                lines.append(f'define {pronobj_tr} "{lemma}+Lemma+{type_tag}+SPLIT+PRONOBJ" : (?* {split_pron_obj_core} ?*) ;')
                tr_names_for_this.append(pronobj_tr)

            if emit_pron_poss:
                pronposs_tr = f"{base}SplitPronPoss"
                lines.append(f'define {pronposs_tr} "{lemma}+Lemma+{type_tag}+SPLIT+PRONPOSS" : (?* {split_pron_poss_core} ?*) ;')
                tr_names_for_this.append(pronposs_tr)

            for norm, pat_name in ext_pat_names:
                ext_tr = f"{base}Ext{norm}"
                lines.append(f'define {ext_tr} "{lemma}+Lemma+{type_tag}+EXT+{norm}" : {pat_name} ;')
                tr_names_for_this.append(ext_tr)

        lines.append("")
        all_transducer_names.extend(tr_names_for_this)

    # =======================
    # 3) Union + save
    # =======================
    lines.append("! =======================")
    lines.append("! Union all lemma transducers")
    lines.append("! =======================")
    if all_transducer_names:
        lines.append("regex " + " | ".join(all_transducer_names) + " ;")
    else:
        lines.append("regex [] ;  ! empty language")

    lines.append("")
    lines.append("! Stack top is generator: Upper=tags, Lower=sentences")
    lines.append("save stack liheci_split.generator.hfst")
    lines.append("")
    lines.append("invert net")
    lines.append("save stack liheci_split.analyser.hfst")
    lines.append("")
    lines.append("quit")
    lines.append("")

    Path(OUTPUT_XFST).write_text("\n".join(lines), encoding="utf-8")
    print(f"[OK] generated {OUTPUT_XFST} with {len(rows)} lemmas; reason_tags={EMIT_REASON_TAGS}")


if __name__ == "__main__":
    main()
